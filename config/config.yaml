# config/config.yaml

# ----------------------------------
# 1. Tokenizer Configuration
# ----------------------------------
tokenizer:
  name: "t5-base"                  # Name or path of the tokenizer/model. E.g., "t5-base" or a local path.
  save_dir: "tokenizers/t5-base"          # Directory where the tokenizer will be saved or loaded from.

# ----------------------------------
# 2. Dataset Configuration
# ----------------------------------
datasets:
  winograd:
    name: "winogrande"
    config: "winogrande_xl"
    path: "datasets/winograd_tokenized.pth"
  
  socialiqa:
    name: "allenai/social_i_qa"
    path: "datasets/socialiqa_tokenized_large"      # Path to save the preprocessed SocialIQA dataset.


# ----------------------------------
# 3. Preprocessing Settings
# ----------------------------------
preprocessing:
  padding: "max_length"          # Padding strategy: "max_length" or "longest".
  truncation: True               # Whether to truncate sequences exceeding max_length.
  max_length: 512                # Maximum token length for Stage 1 inputs.
  max_length_labels: 16         # Maximum token length for Stage 1 labels.
  num_proc: 4                    # Number of parallel processes for preprocessing.


# ----------------------------------
# 4. Training Hyperparameters
# ----------------------------------
training:
  batch_size: 1                           # Optimized batch size.
  learning_rate: 3.0e-5                   # Optimized learning rate.
  dropout_rate: 0.25                      # Optimized dropout rate.
  weight_decay: 0.01                      # Optimized weight decay.
  num_train_epochs: 10                          # Number of training epochs for Stage 1.
  warmup_steps: 500                       # Optimized warmup steps.
  validation_frequency: 5
  max_norm: 1.0
  accumulation_steps: 4

# ----------------------------------
# 5. Logging and Checkpointing
# ----------------------------------
logging:
  log_file: "logs/training_t5_large.log"    # Path to save training and validation logs.

checkpointing:
  save_dir: "checkpoints/t5_large"          # Directory to save and load model checkpoints.
  checkpoint_frequency_batches: 10        # Frequency of checkpoint saving in batches.
  checkpoint_frequency_milestone: 200

# ----------------------------------
# 6. Bayesian Optimization Configuration
# ----------------------------------
bayesian_optimization:
  max_trials: 20                          # Number of trials for optimization.
  tuning_params:
    - name: "learning_rate"
      min: 1.0e-6
      max: 5.0e-5
    - name: "batch_size"
      options: [2, 4, 8]

# ----------------------------------
# 7. Hyperparameter Tuning Configuration
# ----------------------------------
hyperparameter_tuning:
  kfold_splits: 5                # Number of splits for K-Fold cross-validation
  num_trials: 25                 # Number of trials in hyperparameter tuning
  kfold_split_index: 5           # Split index to select during cross-validation
  subset_percentage: 0.035         # Percentage of the dataset to use for training

# ----------------------------------
# 8. Additional Configurations
# ----------------------------------
random_seed: 52  # For reproducibility. You can set this in training.py as well.
scheduler:
  type: "linear"  # Options: "linear", "cosine", etc.
