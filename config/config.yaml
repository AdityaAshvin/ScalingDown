# config/config.yaml

# ----------------------------------
# 1. Tokenizer Configuration
# ----------------------------------
tokenizer:
  name: "t5-base"                  # Name or path of the tokenizer/model. E.g., "t5-base" or a local path.
  save_dir: "tokenizers/t5-base"   # Directory where the tokenizer will be saved or loaded from.

# ----------------------------------
# 2. Dataset Configuration
# ----------------------------------
datasets:
  winograd:
    name: "winogrande"
    config: "winogrande_xl"
    path: "datasets/winograd_tokenized.pth"
  
  socialiqa:
    name: "allenai/social_i_qa"
    path: "datasets/socialiqa_tokenized_large"      # Base path to save the preprocessed SocialIQA dataset.
    splits:
      train: "train"
      validation: "validation"
      test: "test"

# ----------------------------------
# 3. Preprocessing Settings
# ----------------------------------
preprocessing:
  padding: "max_length"          # Padding strategy: "max_length" or "longest".
  truncation: True               # Whether to truncate sequences exceeding max_length.
  max_length: 512                # Maximum token length for inputs.
  max_length_labels: 16          # Maximum token length for labels.
  num_proc: 4                    # Number of parallel processes for preprocessing.

# ----------------------------------
# 4. Training Hyperparameters
# ----------------------------------
training:
  batch_size: 1                           # Optimized batch size.
  learning_rate: 2.6520023461797914e-05  # Optimal learning rate from hyperparameter tuning.
  dropout_rate: 0.10209449828803513       # Optimal dropout rate from hyperparameter tuning.
  weight_decay: 0.03315071292172689       # Optimal weight decay from hyperparameter tuning.
  num_train_epochs_stage2: 10             # Number of training epochs for Stage 2.
  warmup_steps: 500                       # Optimal warmup steps from hyperparameter tuning.
  validation_frequency: 100               # Frequency (in batches) for performing validation.
  max_norm: 1.0                           # Gradient clipping norm.
  accumulation_steps: 4                   # Gradient accumulation steps.
  alpha: 0.5                              # Weighting factor for distillation loss.

# ----------------------------------
# 5. Logging and Checkpointing
# ----------------------------------
logging:
  log_file: "logs/training_flan_t5_large.log"    # Path to save training and validation logs.

checkpointing:
  save_dir: "checkpoints/flan_t5_large"          # Directory to save and load model checkpoints.
  checkpoint_frequency_batches: 10               # Frequency of checkpoint saving in batches.
  checkpoint_frequency_milestone: 200             # Frequency of milestone checkpoint saving in batches.

# ----------------------------------
# 6. Bayesian Optimization Configuration
# ----------------------------------
bayesian_optimization:
  max_trials: 20                          # Number of trials for optimization.
  tuning_params:
    - name: "learning_rate"
      min: 1.0e-6
      max: 5.0e-5
    - name: "batch_size"
      options: [2, 4, 8]

# ----------------------------------
# 7. Hyperparameter Tuning Configuration
# ----------------------------------
hyperparameter_tuning:
  kfold_splits: 5                # Number of splits for K-Fold cross-validation
  num_trials: 45                # Number of trials in hyperparameter tuning
  kfold_split_index: 5           # Split index to select during cross-validation
  subset_percentage: 0.035        # Percentage of the dataset to use for training

# ----------------------------------
# 8. Additional Configurations
# ----------------------------------
random_seed: 52  # For reproducibility. You can set this in training_stage2.py as well.
scheduler:
  type: "linear"  # Options: "linear", "cosine", etc.
