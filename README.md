# ScalingDown
The project is about determining how small a Large Language Model (LLM) can be while still outperforming a larger model on a specific task, using multiple larger models to train. Weâ€™ll use the TinyLLM framework to implement student-teacher models, testing the trade-offs between size, performance, and efficiency
