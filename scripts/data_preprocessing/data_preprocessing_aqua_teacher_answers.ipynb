{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ed34c7696a9d78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Collecting Teacher LLM Answers and Rationales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7014879d8c8c6e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "In this notebook, we will collect answers and rationales from the Llema and GPT-Neo teacher LLMs. The questions previously collected and tokenized via data_preprocessing_aqua_questions.py will be used. The answers and rationales will be collected, a small sample will be checked for correctness and brevity, then the full sample will be collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617aeec01fb36066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T00:00:53.579481Z",
     "start_time": "2024-10-11T00:00:52.432478Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, AutoTokenizer, AutoModelForCausalLM, generate\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d21698",
   "metadata": {},
   "source": [
    "Set the file path, tokenizers, and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf64392f289085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T00:00:53.583646Z",
     "start_time": "2024-10-11T00:00:53.578149Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = 'data/preprocessed/'\n",
    "filename = 'preprocessed_aqua_data.pt'\n",
    "preprocessed_file_path = os.path.join(directory, filename)\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small')\n",
    "llemma_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/llemma_7b\")\n",
    "gptneo_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "\n",
    "llema_model_name = \"EleutherAI/llemma_7b\"\n",
    "llema_model = AutoModelForCausalLM.from_pretrained(llema_model_name)\n",
    "\n",
    "gptneo_model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "gptneo_tokenizer = AutoTokenizer.from_pretrained(gptneo_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df31a1e",
   "metadata": {},
   "source": [
    "Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc9d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llema_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef460f",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    preprocessed_data = torch.load(preprocessed_file_path)\n",
    "    print(\"Preprocessed data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {preprocessed_file_path}. Please check the path and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2425b",
   "metadata": {},
   "source": [
    "Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ddb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_teacher_input = preprocessed_data['train']['teacher_inputs']['llemma'][0]\n",
    "train_teacher_input_llema = preprocessed_data['train']['teacher_inputs']['llemma']\n",
    "train_teacher_input_gptneo = preprocessed_data['train']['teacher_inputs']['gptneo']\n",
    "train_student_inputs = preprocessed_data['train']['student_inputs']\n",
    "\n",
    "assert len(train_teacher_input_llema) == len(train_teacher_input_gptneo) == len(train_student_inputs), \"Data lengths do not match!\"\n",
    "\n",
    "print(f\"Number of training questions for Llema teacher LLMs: {len(train_teacher_input_llema)}\")\n",
    "print(f\"Number of training questions for GPT-Neo teacher LLMs: {len(train_teacher_input_gptneo)}\")\n",
    "print(f\"Number of training questions for student LLM: {len(train_student_inputs)}\")\n",
    "\n",
    "decoded_question = llemma_tokenizer.decode(train_teacher_input_llema[0]['input_ids'].squeeze(), skip_special_tokens=True)\n",
    "print(\"Example of a tokenized question for Llemma:\")\n",
    "print(decoded_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12fd58d",
   "metadata": {},
   "source": [
    "Set the model to evaluation mode since we're just generating responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67952907",
   "metadata": {},
   "outputs": [],
   "source": [
    "llema_model.eval()\n",
    "print(\"Llemma model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb891972",
   "metadata": {},
   "source": [
    "Generate responses for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492dd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tokenized questions\n",
    "llema_responses = []\n",
    "\n",
    "for i, input_data in enumerate(train_teacher_input_llema):\n",
    "    # Move the input tensor to the correct device (GPU or CPU)\n",
    "    input_ids = input_data['input_ids'].to(device)\n",
    "    attention_mask = input_data['attention_mask'].to(device)\n",
    "    \n",
    "    # Generate output using the model\n",
    "    output = llema_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=512,  # Limit response length\n",
    "        num_return_sequences=1,  # Only generate one response per question\n",
    "        do_sample=True,  # Sampling to add randomness for more diverse responses\n",
    "        top_p=0.9,  # Top-p sampling for more controlled outputs\n",
    "        temperature=0.7  # Temperature parameter to adjust randomness\n",
    "    )\n",
    "    \n",
    "    # Decode the generated output\n",
    "    response_text = llemma_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Append the response to the list\n",
    "    llema_responses.append(response_text)\n",
    "    \n",
    "    # Optional: Print progress every 10 questions\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i + 1} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c5bb8",
   "metadata": {},
   "source": [
    "Parse responses, extract answer & rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63084ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of parsing the response to separate answer and rationale\n",
    "answers_rationales = []\n",
    "\n",
    "for response in llema_responses:\n",
    "    # Assume the model responds with: \"Answer: ... Rationale: ...\"\n",
    "    try:\n",
    "        answer_start = response.index(\"Answer: \") + len(\"Answer: \")\n",
    "        rationale_start = response.index(\"Rationale: \") + len(\"Rationale: \")\n",
    "        answer = response[answer_start:rationale_start].strip()\n",
    "        rationale = response[rationale_start:].strip()\n",
    "        \n",
    "        answers_rationales.append({\n",
    "            'answer': answer,\n",
    "            'rationale': rationale\n",
    "        })\n",
    "    except ValueError:\n",
    "        # Handle case where the response does not follow the expected format\n",
    "        answers_rationales.append({\n",
    "            'answer': \"Could not parse answer\",\n",
    "            'rationale': response\n",
    "        })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsci566)",
   "language": "python",
   "name": "dsci566"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
